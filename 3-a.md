**Conclusion:** Adaptive timeout helps, but doesn't solve fundamental bufferbloat problem. Moderate buffer sizes with AQM work better than very large buffers.

---

## P31. RTT Estimation

**Given:**
- SampleRTT values: 106, 120, 140, 90, 115 ms
- α = 0.125, β = 0.25
- Initial: EstimatedRTT = 100 ms, DevRTT = 5 ms

**Formulas:**
```
EstimatedRTT = (1-α) × EstimatedRTT + α × SampleRTT
DevRTT = (1-β) × DevRTT + β × |SampleRTT - EstimatedRTT|
TimeoutInterval = EstimatedRTT + 4 × DevRTT
```

**Calculations:**

**After SampleRTT₁ = 106 ms:**
```
EstimatedRTT₁ = 0.875 × 100 + 0.125 × 106
              = 87.5 + 13.25 = 100.75 ms

DevRTT₁ = 0.75 × 5 + 0.25 × |106 - 100|
        = 3.75 + 0.25 × 6
        = 3.75 + 1.5 = 5.25 ms

TimeoutInterval₁ = 100.75 + 4 × 5.25
                 = 100.75 + 21 = 121.75 ms
```

**After SampleRTT₂ = 120 ms:**
```
EstimatedRTT₂ = 0.875 × 100.75 + 0.125 × 120
              = 88.156 + 15 = 103.156 ms

DevRTT₂ = 0.75 × 5.25 + 0.25 × |120 - 100.75|
        = 3.9375 + 0.25 × 19.25
        = 3.9375 + 4.8125 = 8.75 ms

TimeoutInterval₂ = 103.156 + 4 × 8.75
                 = 103.156 + 35 = 138.156 ms
```

**After SampleRTT₃ = 140 ms:**
```
EstimatedRTT₃ = 0.875 × 103.156 + 0.125 × 140
              = 90.262 + 17.5 = 107.762 ms

DevRTT₃ = 0.75 × 8.75 + 0.25 × |140 - 103.156|
        = 6.5625 + 0.25 × 36.844
        = 6.5625 + 9.211 = 15.774 ms

TimeoutInterval₃ = 107.762 + 4 × 15.774
                 = 107.762 + 63.096 = 170.858 ms
```

**After SampleRTT₄ = 90 ms:**
```
EstimatedRTT₄ = 0.875 × 107.762 + 0.125 × 90
              = 94.292 + 11.25 = 105.542 ms

DevRTT₄ = 0.75 × 15.774 + 0.25 × |90 - 107.762|
        = 11.831 + 0.25 × 17.762
        = 11.831 + 4.441 = 16.272 ms

TimeoutInterval₄ = 105.542 + 4 × 16.272
                 = 105.542 + 65.088 = 170.630 ms
```

**After SampleRTT₅ = 115 ms:**
```
EstimatedRTT₅ = 0.875 × 105.542 + 0.125 × 115
              = 92.349 + 14.375 = 106.724 ms

DevRTT₅ = 0.75 × 16.272 + 0.25 × |115 - 105.542|
        = 12.204 + 0.25 × 9.458
        = 12.204 + 2.365 = 14.569 ms

TimeoutInterval₅ = 106.724 + 4 × 14.569
                 = 106.724 + 58.276 = 164.999 ms
```

**Summary Table:**

| Sample | SampleRTT | EstimatedRTT | DevRTT | TimeoutInterval |
|--------|-----------|--------------|---------|-----------------|
| Initial| -         | 100.000      | 5.000   | 120.000         |
| 1      | 106       | 100.750      | 5.250   | 121.750         |
| 2      | 120       | 103.156      | 8.750   | 138.156         |
| 3      | 140       | 107.762      | 15.774  | 170.858         |
| 4      | 90        | 105.542      | 16.272  | 170.630         |
| 5      | 115       | 106.724      | 14.569  | 165.000         |

---

## P32. RTT Estimation Formula

**Given:** α = 0.1

**Formula:**
```
EstimatedRTTₙ = (1-α) × EstimatedRTTₙ₋₁ + α × SampleRTTₙ
```

**a. Express EstimatedRTT in terms of 4 most recent samples**

Expanding recursively:

```
EstimatedRTTₙ = 0.9 × EstimatedRTTₙ₋₁ + 0.1 × SampleRTTₙ

EstimatedRTTₙ₋₁ = 0.9 × EstimatedRTTₙ₋₂ + 0.1 × SampleRTTₙ₋₁

EstimatedRTTₙ₋₂ = 0.9 × EstimatedRTTₙ₋₃ + 0.1 × SampleRTTₙ₋₂

EstimatedRTTₙ₋₃ = 0.9 × EstimatedRTTₙ₋₄ + 0.1 × SampleRTTₙ₋₃
```

Substituting:

```
EstimatedRTTₙ = 0.9(0.9 × EstimatedRTTₙ₋₂ + 0.1 × SampleRTTₙ₋₁) + 0.1 × SampleRTTₙ
              = 0.81 × EstimatedRTTₙ₋₂ + 0.09 × SampleRTTₙ₋₁ + 0.1 × SampleRTTₙ

EstimatedRTTₙ = 0.81(0.9 × EstimatedRTTₙ₋₃ + 0.1 × SampleRTTₙ₋₂) + 0.09 × SampleRTTₙ₋₁ + 0.1 × SampleRTTₙ
              = 0.729 × EstimatedRTTₙ₋₃ + 0.081 × SampleRTTₙ₋₂ + 0.09 × SampleRTTₙ₋₁ + 0.1 × SampleRTTₙ

EstimatedRTTₙ = 0.729(0.9 × EstimatedRTTₙ₋₄ + 0.1 × SampleRTTₙ₋₃) + 0.081 × SampleRTTₙ₋₂ + 0.09 × SampleRTTₙ₋₁ + 0.1 × SampleRTTₙ
              = 0.6561 × EstimatedRTTₙ₋₄ + 0.0729 × SampleRTTₙ₋₃ + 0.081 × SampleRTTₙ₋₂ + 0.09 × SampleRTTₙ₋₁ + 0.1 × SampleRTTₙ
```

**Answer:**
```
EstimatedRTTₙ = 0.6561 × EstimatedRTTₙ₋₄ 
              + 0.0729 × SampleRTTₙ₋₃ 
              + 0.081 × SampleRTTₙ₋₂ 
              + 0.09 × SampleRTTₙ₋₁ 
              + 0.1 × SampleRTTₙ

Or: = 0.9⁴ × EstimatedRTTₙ₋₄ + 0.1 × Σᵢ₌₀³ (0.9ⁱ × SampleRTTₙ₋ᵢ)
```

**b. Generalize for n samples**

**Answer:**
```
EstimatedRTTₙ = (1-α)ⁿ × EstimatedRTT₀ + α × Σᵢ₌₀ⁿ⁻¹ [(1-α)ⁱ × SampleRTTₙ₋ᵢ]

For α = 0.1:
EstimatedRTTₙ = 0.9ⁿ × EstimatedRTT₀ + 0.1 × Σᵢ₌₀ⁿ⁻¹ [0.9ⁱ × SampleRTTₙ₋ᵢ]
```

**c. For n → ∞, explain "exponential moving average" name**

**Analysis:**

As n → ∞:
- The term (1-α)ⁿ → 0 (for 0 < α < 1)
- Initial EstimatedRTT₀ influence vanishes
- Left with: EstimatedRTT = α × Σᵢ₌₀^∞ [(1-α)ⁱ × SampleRTTₙ₋ᵢ]

**Why "exponential":**
- Weights decay exponentially: (1-α)⁰, (1-α)¹, (1-α)², (1-α)³, ...
- Recent samples weighted more heavily
- Weight decreases by factor of (1-α) for each older sample

**Why "moving average":**
- Continuously updates as new samples arrive
- Window effectively "moves" forward in time
- Averages all past samples (with exponentially decreasing weights)

**Geometric series:**
```
Σᵢ₌₀^∞ [(1-α)ⁱ] = 1/(1-(1-α)) = 1/α

Weights sum to 1 (normalized):
α × 1/α = 1 ✓
```

**Example with α = 0.1:**
- Most recent sample: weight = 0.1
- 1 sample ago: weight = 0.09
- 2 samples ago: weight = 0.081
- 3 samples ago: weight = 0.0729
- Exponential decay!

This is why it's called **Exponential Weighted Moving Average (EWMA)** or **Exponential Moving Average (EMA)**.

---

## P33. Why TCP Avoids Measuring SampleRTT for Retransmitted Segments

**Problem:** Ambiguity in RTT measurement for retransmitted segments.

**Scenario:**

```
Time    Sender                           Receiver
─────────────────────────────────────────────────────
t0      Send SEG (seq=100)
        Start timer
        ────────────────────────X (lost)
        
t1      Timeout
        Retransmit SEG (seq=100)
        Start timer again
        ──────────────────────────────> Receive SEG
                                        Send ACK
        <────────────────────────────── 
t2      Receive ACK
```

**Ambiguity Question:** When ACK arrives at t2, which transmission does it acknowledge?

**Case 1:** ACK is for original transmission (which actually arrived, but we thought it was lost)
- True RTT = t2 - t0 (long)
- If we measure from retransmission: t2 - t1 (short) → underestimate

**Case 2:** ACK is for retransmission (original really was lost)
- True RTT = t2 - t1 (correct)
- If we measure from original: t2 - t0 (long) → overestimate

**Cannot distinguish between:**
1. Premature timeout → original packet delayed → ACK for original
2. Actual loss → retransmission successful → ACK for retransmission

**Consequences of wrong measurement:**

**If underestimate RTT:**
- RTO becomes too short
- More premature timeouts
- Unnecessary retransmissions
- Congestion increases

**If overestimate RTT:**
- RTO becomes too long
- Slow to detect actual losses
- Poor performance

**Solution:** **Karn's Algorithm**
- Don't measure SampleRTT for retransmitted segments
- Only measure RTT for segments that are ACKed without retransmission
- For retransmissions: double RTO (exponential backoff)

This avoids the ambiguity problem entirely.

---

## P34. Relationship: SendBase and LastByteRcvd

**Definitions:**

**SendBase (Section 3.5.4 - Sender side):**
- Sequence number of oldest unacknowledged byte
- Base of sender's window
- All bytes before SendBase have been ACKed

**LastByteRcvd (Section 3.5.5 - Receiver side):**
- Sequence number of last byte received and placed in receiver buffer
- May have gaps before it (out-of-order arrivals)

**Relationship:**

```
SendBase ≤ LastByteRcvd + 1
```

**Analysis:**

1. **In perfect conditions (no loss, no reordering):**
   ```
   SendBase = LastByteRcvd + 1
   ```
   - Receiver has received all bytes up to LastByteRcvd
   - Receiver sends cumulative ACK for LastByteRcvd + 1
   - Sender moves SendBase to LastByteRcvd + 1

2. **With packet loss:**
   ```
   SendBase < LastByteRcvd + 1
   ```
   - Sender has sent bytes beyond SendBase
   - Some of these bytes received (LastByteRcvd moved forward)
   - But earlier bytes (at/after SendBase) lost
   - Receiver cannot ACK beyond gap
   - SendBase stuck until lost packet retransmitted

3. **With ACK loss:**
   ```
   SendBase < LastByteRcvd + 1
   ```
   - Receiver got all bytes, moved LastByteRcvd forward
   - Sent ACK, but ACK lost
   - Sender's SendBase not updated yet
   - When duplicate ACK or next ACK arrives, SendBase catches up

**Example:**
```
Sender sends bytes: 1-10
Byte 3 lost

Receiver gets: 1,2,4,5,6,7,8,9,10
LastByteRcvd = 10 (last byte received)
But can only ACK = 3 (cumulative, waiting for byte 3)

Sender:
SendBase = 3 (waiting for ACK beyond 2)
SendBase (3) << LastByteRcvd (10) + 1
```

**Summary:** SendBase ≤ LastByteRcvd + 1, with equality in ideal conditions and inequality with packet/ACK loss or reordering.

---

## P35. Relationship: LastByteRcvd and Variable y

**Definitions:**

**LastByteRcvd (Section 3.5.5):**
- Sequence number of last byte in-order received
- Actually, more precisely: last byte of in-order data at receiver

**Variable y (Section 3.5.4 - typically refers to receiver window):**
- In the context of flow control
- y = last byte that receiver is willing to accept
- Related to receive window: y = LastByteRead + RcvBuffer

Wait, let me clarify. In typical TCP notation:

**LastByteRead:** Last byte read by application
**LastByteRcvd:** Last byte received (in order)
**RcvBuffer:** Size of receive buffer

**Relationship:**

```
LastByteRcvd ≤ LastByteRead + RcvBuffer
```

And receiver advertises window:
```
rwnd = RcvBuffer - (LastByteRcvd - LastByteRead)
```

If y represents the upper edge of receive window:
```
y = LastByteRcvd + rwnd
  = LastByteRcvd + RcvBuffer - (LastByteRcvd - LastByteRead)
  = LastByteRead + RcvBuffer
```

**So:**
```
LastByteRcvd ≤ y
y = LastByteRead + RcvBuffer

And specifically:
LastByteRcvd = LastByteRead + (bytes in buffer)
y - LastByteRcvd = available buffer space = rwnd
```

**In words:** y is the upper bound of what receiver can accept. LastByteRcvd is current position. The difference (y - LastByteRcvd) is the advertised receive window.

---

## P36. Why TCP Doesn't Fast Retransmit After First Duplicate ACK

**Fast Retransmit:** TCP retransmits after receiving 3 duplicate ACKs (total of 4 identical ACKs).

**Why not retransmit after 1st duplicate ACK?**

**Reason 1: Packet Reordering**
- Internet can deliver packets out of order
- Segment n+1 might arrive before segment n due to different routing
- Receiver sends duplicate ACK (still waiting for n)
- This is not a loss, just reordering!
- Retransmitting immediately wastes bandwidth

**Example:**
```
Sender sends: PKT1, PKT2, PKT3

Network reorders: PKT1 → PKT3 → PKT2

Receiver:
- Gets PKT1 → sends ACK2
- Gets PKT3 (out of order!) → sends duplicate ACK2
- Gets PKT2 → sends ACK4

If retransmit after 1st duplicate ACK:
- Waste: retransmit PKT2 unnecessarily
```

**Reason 2: Statistical Threshold**
- Single duplicate ACK could be spurious
- 3 duplicates strongly suggest loss (probability of 3-position reordering is low)
- Threshold balances:
  - **Too low (1):** too sensitive, many false positives
  - **Too high (>3):** slow recovery, unnecessary waiting
  - **3 duplicates:** good empirical balance

**Reason 3: Give Network Time**
- First duplicate: might be reordering
- Second duplicate: probably reordering
- Third duplicate: very likely loss
- Waiting for 3 gives reordered packets time to arrive

**Statistics:**
- Most packet reordering is within 1-2 positions
- 3-position reordering is rare
- 3 duplicate ACKs = high confidence of loss

**Alternative Considered:** Retransmit after 1 duplicate
- **Problem:** Massive waste of bandwidth due to reordering
- **Studies:** Show significant performance degradation

**Conclusion:** 3 duplicate ACKs is empirically optimal threshold, balancing fast recovery against spurious retransmissions from reordering.

---

## P37. Compare GBN, SR, and TCP

**Scenario:**
- Host A sends 5 segments to Host B
- 2nd segment lost
- Eventually all 5 received correctly
- Timeout ≫ 5 × RTT (large enough that fast retransmit happens first in TCP)

**a. Segments and ACKs sent, their sequence numbers:**

**Go-Back-N (GBN):**

```
Segments sent by A:
1. SEG1 (seq=0)
2. SEG2 (seq=1) ────X (lost)
3. SEG3 (seq=2)
4. SEG4 (seq=3)
5. SEG5 (seq=4)
[Timeout or receive duplicate ACKs]
6. SEG2 (seq=1) - retransmit
7. SEG3 (seq=2) - retransmit
8. SEG4 (seq=3) - retransmit
9. SEG5 (seq=4) - retransmit

Total: 9 segments

ACKs sent by B:
1. ACK1 (after receiving SEG1)
2. ACK1 (after receiving SEG3, still waiting for SEG2)
3. ACK1 (after receiving SEG4, still waiting for SEG2)
4. ACK1 (after receiving SEG5, still waiting for SEG2)
5. ACK5 (after receiving retransmitted SEG2,3,4,5 in order)

Total: 5 ACKs
```

**Selective Repeat (SR):**

```
Segments sent by A:
1. SEG1 (seq=0)
2. SEG2 (seq=1) ────X (lost)
3. SEG3 (seq=2)
4. SEG4 (seq=3)
5. SEG5 (seq=4)
[After timeout for SEG2 or receiving selective ACK]
6. SEG2 (seq=1) - retransmit only this one

Total: 6 segments

ACKs sent by B:
1. ACK0 (after receiving SEG1)
2. ACK2 (after receiving SEG3, selective ACK)
3. ACK3 (after receiving SEG4, selective ACK)
4. ACK4 (after receiving SEG5, selective ACK)
5. ACK4 or cumulative ACK5 (after receiving retransmitted SEG2)

Total: 5 ACKs
```

**TCP:**

```
Segments sent by A:
1. SEG1 (seq=0)
2. SEG2 (seq=1) ────X (lost)
3. SEG3 (seq=2)
4. SEG4 (seq=3)
5. SEG5 (seq=4)
[After 3 duplicate ACKs, fast retransmit]
6. SEG2 (seq=1) - fast retransmit

Total: 6 segments

ACKs sent by B:
1. ACK1 (after receiving SEG1)
2. ACK1 (duplicate, after receiving SEG3)
3. ACK1 (duplicate, after receiving SEG4)
4. ACK1 (duplicate, after receiving SEG5) [3rd duplicate!]
5. ACK5 (cumulative, after receiving retransmitted SEG2)

Total: 5 ACKs
```

**b. Which protocol delivers data in shortest time?**

**Assumptions:**
- Timeout ≫ 5 × RTT
- Fast retransmit happens before timeout

**Timeline Analysis:**

**GBN:**
```
t=0:     Send SEG1
t=RTT:   Receive ACK1, send SEG2 (lost)
t=2RTT:  Send SEG3, receive duplicate ACK1
t=3RTT:  Send SEG4, receive duplicate ACK1
t=4RTT:  Send SEG5, receive duplicate ACK1
t=5RTT:  Receive duplicate ACK1
         [Need timeout since GBN doesn't fast retransmit]
t=Timeout: Retransmit SEG2,3,4,5
t=Timeout+4RTT: All received

Total: Timeout + 4RTT (very long!)
```

**SR:**
```
t=0:     Send SEG1
t=RTT:   Receive ACK0, send SEG2 (lost)
t=2RTT:  Send SEG3, receive ACK2 (selective)
t=3RTT:  Send SEG4, receive ACK3
t=4RTT:  Send SEG5, receive ACK4
t=Timeout for SEG2: Retransmit SEG2
t=Timeout+RTT: Receive ACK for SEG2, all done

Total: Timeout + RTT
```

**TCP (with fast retransmit):**
```
t=0:     Send SEG1
t=RTT:   Receive ACK1, send SEG2 (lost)
t=2RTT:  Send SEG3, receive duplicate ACK1
t=3RTT:  Send SEG4, receive duplicate ACK1
t=4RTT:  Send SEG5, receive duplicate ACK1 (3rd dup!)
         Fast retransmit SEG2
t=5RTT:  Receive ACK5 (cumulative)

Total: 5RTT
```

**Answer: TCP is fastest** (5 RTT), followed by SR (depends on timeout), then GBN (slowest due to retransmitting all subsequent packets).

**Ranking:**
1. **TCP:** 5 RTT (with fast retransmit)
2. **SR:** Timeout + RTT (if timeout < 4RTT, better than TCP; typically yes)
3. **GBN:** Timeout + 4RTT (slowest)

Actually, if SR's timeout is set properly (≈ 2RTT for SEG2), then:
- **SR:** ≈ 3RTT (fastest!)
- **TCP:** 5RTT
- **GBN:** Much longer

**Revised answer: SR is fastest with proper timeout**, TCP second, GBN slowest.

---

## P38. TCP Congestion Control - ssthresh Setting

**Question:** When ssthresh = cwnd/2, is sender's rate necessarily cwnd segments/RTT?

**Answer: NO**

**Reasons:**

1. **Flow Control Limitation:**
   ```
   Effective window = min(cwnd, rwnd)
   ```
   - If rwnd < cwnd, sending rate limited by receiver's window
   - Rate = min(cwnd, rwnd) / RTT, not necessarily cwnd/RTT

2. **Network Buffering:**
   - Even with large cwnd, network might have queued packets
   - Actual sending rate limited by ACK arrival rate
   - ACK clocking effect

3. **Application Limitation:**
   - Application might not have data to send
   - Can't send cwnd segments if application produces fewer

4. **Delayed ACKs:**
   - Receiver might delay ACKs
   - Slows window advancement
   - Reduces effective rate

**Better way to set ssthresh:**

Instead of ssthresh = cwnd/2 at time of loss detection, use:

**ssthresh = (bytes in flight at loss) / 2**

Or more accurately:

**ssthresh = max(FlightSize/2, 2×MSS)**

Where FlightSize = number of unacknowledged bytes.

**Why better:**
- Reflects actual network usage, not just cwnd value
- If flow control limited sending, cwnd/2 is too optimistic
- FlightSize/2 better estimates actual capacity before congestion

**Even better: BW estimation approaches:**
- Measure actual achieved throughput
- Set ssthresh based on measured bandwidth-delay product
- TCP Westwood, TCP Vegas use bandwidth estimation

**Example:**
```
cwnd = 100 segments
rwnd = 40 segments
Actual flight size = 40 segments
Loss detected

Bad: ssthresh = cwnd/2 = 50 (too high!)
Better: ssthresh = FlightSize/2 = 20 (more realistic)
```

**Conclusion:** cwnd/2 is simple but imperfect. Using actual flight size or bandwidth estimation provides better ssthresh values.

---

## P39. Router Throughput

**Notation:**
- l'ᵢₙ = average input rate
- R = link capacity  
- lₒᵤₜ = average output rate (throughput)

**a. If l'ᵢₙ > R/2, can lₒᵤₜ > R/3?**

**Answer: NO (not guaranteed, depends on traffic pattern)**

**Analysis:**

If packets require forwarding to multiple outputs (multicast/broadcast):
- Input rate l'ᵢₙ > R/2
- Each input packet might need copying to multiple outputs
- Total output load could exceed R
- Queuing and drops occur
- Actual throughput lₒᵤₜ could be limited

But with **unicast** and **simple forwarding**:
- Input rate = output rate (in steady state)
- If l'ᵢₙ > R/2, and only one output link
- lₒᵤₜ should equal l'ᵢₙ (up to capacity R)
- So lₒᵤₜ > R/2 > R/3 ✓

**However, the question is suspicious. Let me reconsider:**

If there are **multiple output links**:
- l'ᵢₙ > R/2 is aggregate input
- If distributed evenly across 3+ output links
- Each output might get l'ᵢₙ/3 < R/3 if l'ᵢₙ < R

**Without more context, answer depends on:**
- Number of output links
- Traffic distribution
- Forwarding pattern

**Conservative answer:** **Cannot guarantee lₒᵤₜ > R/3** without knowing traffic distribution.

**b. If l'ᵢₙ > R/2, can lₒᵤₜ > R/4 (assuming average packet forwarded twice)?**

**Answer: YES, likely**

**Analysis:**

"Average packet forwarded twice" suggests:
- Packet enters router
- Gets forwarded to two output links (multicast or routing)

**Load analysis:**
- Input load: l'ᵢₙ
- Each packet forwarded twice
- Total output load = 2 × l'ᵢₙ

If l'ᵢₙ > R/2:
- Output load = 2 × l'ᵢₙ > R
- Router is overloaded!
- Some packets will be dropped
- But lₒᵤₜ can still be substantial

**Can lₒᵤₜ > R/4?**
- If l'ᵢₙ = 0.6R (satisfies l'ᵢₙ > R/2)
- Desired output = 2 × 0.6R = 1.2R
- Actual output capacity = R
- lₒᵤₜ ≈ R (capacity limit)
- R > R/4 ✓ **YES**

Even with losses, the router can sustain lₒᵤₜ approaching R, which is greater than R/4.

**Answer: YES, lₒᵤₜ can exceed R/4**

---

## P41. AIAD vs AIMD

**Question:** Would TCP converge to fairness with **Additive Increase, Additive Decrease (AIAD)** instead of AIMD?

**AIAD:**
- Additive Increase: cwnd = cwnd + α (on no loss)
- Additive Decrease: cwnd = cwnd - β (on loss)

**AIMD (actual TCP):**
- Additive Increase: cwnd = cwnd + α  
- Multiplicative Decrease: cwnd = cwnd × (1-β), typically cwnd = cwnd/2

**Analysis with Diagram:**

Let's consider two connections sharing a bottleneck link with capacity C.

**AIMD Behavior (Figure 3.55 style):**

```
           cwnd₂
            ↑
            │     Fairness line
            │    (x₁ = x₂)
            │   /
        C   │  /
            │ /
            │/____________ Efficiency line
            │            (x₁ + x₂ = C)
            │          /
            │        /
            │      /  ← AI moves parallel to fairness
            │    /      line
            │  /   
            │/  ↖ MD moves toward fairness line
            └───────────────→ cwnd₁
                  C

Starting point: (cwnd₁, cwnd₂) unfair

Additive Increase (no loss):
- Both increase by α
- Vector: (+α, +α)
- Moves parallel to fairness line
- Maintains unfairness ratio

Multiplicative Decrease (loss):
- Both decrease by factor β
- New point: (βcwnd₁, βcwnd₂)
- Vector toward origin, crosses toward fairness line
- Reduces unfairness
```

**AIAD Behavior:**

```
           cwnd₂
            ↑
            │     Fairness line
            │    (x₁ = x₂)
            │   /
        C   │  /
            │ /
            │/____________ Efficiency line
            │            (x₁ + x₂ = C)
            │          /
            │        /
            │      /  ← AI moves parallel
            │    /   
            │  /   ↙ AD also parallel to fairness!
            │/      Does NOT move toward fairness
            └───────────────→ cwnd₁
                  C

Starting point: (cwnd₁, cwnd₂) unfair

Additive Increase (no loss):
- Vector: (+α, +α)
- Parallel to fairness line

Additive Decrease (loss):
- Vector: (-β, -β)  
- STILL parallel to fairness line!
- Does NOT reduce unfairness
```

**Key Insight:**

With AIAD, both increase and decrease are **parallel to the fairness line**, so the system never converges toward fairness.

**Mathematical Proof:**

Let unfairness ratio: r = cwnd₁/cwnd₂

**After AI:** 
r' = (cwnd₁ + α)/(cwnd₂ + α)

If cwnd₁ > cwnd₂, then r > 1, and r' < r (moves toward fairness slightly)

**After AD:**
r' = (cwnd₁ - β)/(cwnd₂ - β)

If cwnd₁ > cwnd₂, then r' > r (moves AWAY from fairness!)

The AD step moves away from fairness, canceling AI's progress.

**With AIMD:**

**After AI:**
r' = (cwnd₁ + α)/(cwnd₂ + α) → closer to 1

**After MD:**
r' = (γ × cwnd₁)/(γ × cwnd₂) = cwnd₁/cwnd₂ = r (preserves ratio)

Then AI again improves fairness. Net effect: convergence to fairness.

**Conclusion: NO, AIAD does not converge to fairness.** Only AIMD (or variants with multiplicative decrease) converges to fairness because MD preserves ratios while AI improves them.

---

## P42. Why Window-Based Congestion Control in Addition to Timeout Doubling?

**Context:** TCP already doubles timeout interval after each timeout (exponential backoff).

**Why is this insufficient? Why need window-based control?**

**Answer: Timeout doubling alone is too slow and reactive.**

**Problems with timeout-only approach:**

1. **Slow response to congestion:**
   - Timeout doubling only activates after loss
   - By then, severe congestion may have occurred
   - Doesn't prevent congestion, only reacts to it

2. **Binary response:**
   - Either send at full rate or wait for timeout
   - No gradual adjustment
   - Can't find optimal sending rate

3. **Poor network utilization:**
   ```
   Normal rate → Loss → Long timeout → Resume full rate → Loss again
   ```
   - Oscillates between full rate and idle
   - Wastes network capacity during timeout
   - No smooth adaptation

4. **Doesn't prevent congestion collapse:**
   - Multiple flows could simultaneously send at high rates
   - All timeout, all resume together
   - Synchronized oscillation
   - Repeated congestion

5. **No probing for available bandwidth:**
   - Timeout only tells you "too much was sent"
   - Doesn't help find right amount
   - Window-based probing gradually increases to find capacity

**Window-based control provides:**

1. **Proactive prevention:**
   - Gradually increases sending rate (AI)
   - Finds capacity before severe congestion

2. **Smooth adaptation:**
   - Continuously adjusts to network conditions
   - Fine-grained control

3. **Fast recovery:**
   - Fast retransmit/recovery maintains some transmission
   - Doesn't go fully idle
   - Better utilization

4. **Fairness:**
   - AIMD converges to fair allocation
   - Timeout doubling doesn't provide fairness

5. **Responsive to ACKs:**
   - ACK clocking provides fine-grained feedback
   - Adjusts every RTT, not just at timeouts

**Example:**

```
Timeout-only:
Time: 0────────1────────2────────3────────4────
Rate: ▓▓▓▓▓▓▓▓░░░░░░░░░▓▓▓▓▓▓▓▓░░░░░░░░░▓▓▓▓
      Full    Timeout   Full    Timeout   ...
      Wasteful, oscillating

Window-based:
Time: 0────────1────────2────────3────────4────
Rate: ▓░░░▓▓░░▓▓▓░▓▓▓▓░▓▓▓░░▓▓▓▓░▓▓▓▓▓░▓▓▓▓▓
      Smooth probing, better utilization
```

**Conclusion:** Timeout doubling is a backup mechanism for severe congestion/loss. Window-based control is primary mechanism for: preventing congestion, finding optimal rate, maintaining fairness, and maximizing utilization.

---

## P43. Flow Control Limits

**Setup:**
- A → B over perfect TCP (no loss)
- A's application rate S = 10 × R (10 times link rate)
- Send buffer = 1% of file size
- Receive buffer is large

**Question:** What prevents continuous 10×R sending?

**Analysis:**

**Not Congestion Control:**
- Network is perfect (no loss)
- Congestion control won't kick in
- cwnd will grow to maximum

**Not Flow Control (directly):**
- Receive buffer is large
- B can accommodate incoming data
- rwnd stays large

**Answer: The Send Buffer!**

**Explanation:**

1. **Send buffer fills quickly:**
   - Application produces data at 10R
   - Network sends at R
   - Send buffer accumulates at rate: 10R - R = 9R
   - Buffer size = 1% of file
   - Fills in time: t = (0.01 × file_size) / 9R

2. **Application blocks:**
   - Once send buffer is full
   - `send()` system call blocks
   - Application cannot write more data
   - Forced to wait until buffer space available

3. **Steady state:**
   - Send buffer alternates: full ↔ partially full
   - Application blocks whenever buffer full
   - Effective application rate throttled to R
   - Matches network capacity

**Timeline:**

```
Time          Send Buffer          App Rate    Network Rate
────────────────────────────────────────────────────────────
t=0           Empty               10R          R
              ↓ fills              ↓            ↓
t=0.01×File/9R  Full              0 (blocked)  R
              ↓ drains             ↓            ↓
              Partial             10R           R
              ↓ fills              ↓            ↓
              Full                0 (blocked)   R
              
Oscillates, average ≈ R
```

**What limits continuous sending:**

**Primary:** **Send buffer capacity** + **application blocking**

**Mechanism:**
1. Socket send buffer has limited size
2. TCP won't accept data from application if send buffer full
3. Application's write() calls block
4. Forces application to slow down

**Secondary factors:**
- If send buffer were infinite, then flow control (rwnd) might limit
- But question states rwnd is large
- So send buffer is the bottleneck

**Conclusion:** The **send buffer size** (1% of file) prevents continuous 10×R sending by blocking the application when full. This is neither flow control (receiver-side) nor congestion control (network-side), but **socket buffer management** at the sender.

---

## P44. TCP AIMD

**Given:**
- cwnd increases by 1 MSS per RTT (additive increase)
- Currently: cwnd = 6 MSS

**a. How long to grow from 6 MSS to 12 MSS?**

In congestion avoidance phase (additive increase):
- Each RTT: cwnd increases by 1 MSS
- Start: 6 MSS
- Target: 12 MSS
- Increase needed: 12 - 6 = 6 MSS

**Time = 6 RTTs**

**Answer: 6 RTT**

**b. Average throughput up to 6 RTTs?**

Window sizes over 6 RTTs:
```
RTT 0: cwnd = 6 MSS
RTT 1: cwnd = 7 MSS  
RTT 2: cwnd = 8 MSS
RTT 3: cwnd = 9 MSS
RTT 4: cwnd = 10 MSS
RTT 5: cwnd = 11 MSS
RTT 6: cwnd = 12 MSS (reached)
```

Wait, need to clarify: "up to 6 RTTs" means during the first 6 RTTs of growth.

During each RTT, throughput ≈ cwnd / RTT

```
RTT 1: throughput = 6 MSS / RTT
RTT 2: throughput = 7 MSS / RTT
RTT 3: throughput = 8 MSS / RTT
RTT 4: throughput = 9 MSS / RTT
RTT 5: throughput = 10 MSS / RTT
RTT 6: throughput = 11 MSS / RTT
```

Average throughput:
```
Avg = (6 + 7 + 8 + 9 + 10 + 11) / 6
    = 51 / 6
    = 8.5 MSS/RTT
```

**Answer: 8.5 MSS per RTT**

Or in bytes/second: **8.5 × MSS / RTT**

---

## P45-P46. TCP Reno vs CUBIC

**Scenario:**
- Congestion occurs at window size W
- Loss detected at 0.75 × W_max
- Later, loss detected at 1.5 × W_max (capacity increased)
- Show evolution for two rounds each

**TCP Reno:**

**Round 1: Loss at 0.75 × W_max**

Assume W_max = 100 MSS, so loss at 75 MSS

```
Initial cwnd (after previous loss): 50 MSS (assume)

Additive Increase:
51, 52, 53, ..., 75 (loss detected)

Multiplicative Decrease:
cwnd = 75 / 2 = 37.5 ≈ 38 MSS
ssthresh = 38 MSS

Resume AI:
38, 39, 40, 41, ... (continuing)
```

**Round 2: Loss at 1.5 × W_max = 150 MSS**

```
Continuing from 38:
38, 39, 40, ..., 150 (loss detected)

MD:
cwnd = 150 / 2 = 75 MSS
ssthresh = 75 MSS

Resume AI:
75, 76, 77, ... (continuing)
```

**TCP CUBIC:**

CUBIC uses cubic function: W(t) = C(t - K)³ + W_max

Where K = ∛(W_max × β / C), β = 0.7, C = constant

**Round 1: Loss at 0.75 × W_max**

```
W_max previously was, say, 100 MSS
Loss at 75 MSS

MD:
cwnd = 75 × 0.7 = 52.5 ≈ 53 MSS

CUBIC growth:
W_max = 75 (update to loss point)
K = ∛((75 × 0.3) / C) 

Growth follows cubic curve toward 75:
53 → 57 → 63 → 69 → 74 → 75 (fast initial growth)
Then continues: 76 → 78 → 80 (probing beyond W_max)
```

**Round 2: Loss at 1.5 × W_max = 150 MSS**

```
Loss at 150 MSS

MD:
cwnd = 150 × 0.7 = 105 MSS

CUBIC growth:
W_max = 150 (update)
K = ∛((150 × 0.3) / C)

Growth:
105 → 115 → 125 → 135 → 145 → 150 (plateau region)
Then: 152 → 155 → 159 (probing beyond)
```

**Key Differences:**

1. **Reno:** Linear growth (AI), 50% reduction (MD)
2. **CUBIC:** 
   - Cubic growth curve
   - 30% reduction (β = 0.7)
   - Fast recovery to previous W_max
   - Slow probing around W_max (plateau)
   - Aggressive probing beyond W_max

**Graphs:**

```
Reno:
cwnd
  │     /
  │    /
  │   /  ← Linear AI
  │  /
  │ /
  │/
  └─────────── time
   ↓ MD (÷2)

CUBIC:
cwnd
  │      __,--
  │   _-'    ← Cubic curve
  │ _/
  │/
  └─────────── time
   ↓ MD (×0.7)
```

---

## P47. TCP Loss Rate Formula

**a. Show: L = 1 / (3/8 × W² + 3/4 × W)**

Where:
- L = loss probability
- W = maximum window size (at loss)

**Derivation:**

TCP Reno behavior:
- Window grows from W/2 to W (additive increase)
- Loss occurs at W
- Window drops to W/2 (multiplicative decrease)

**Packets sent in one cycle:**
```
Windows: W/2, W/2+1, W/2+2, ..., W-1, W

Number of windows = W - W/2 + 1 = W/2 + 1

Packets sent = Σᵢ₌₀^(W/2) (W/2 + i)
             = (W/2 + 1) × W/2 + Σᵢ₌₀^(W/2) i
             = (W/2 + 1) × W/2 + (W/2)(W/2 + 1)/2
             = (W/2 + 1)[W/2 + W/4]
             = (W/2 + 1) × 3W/4
             ≈ (W/2) × (3W/4)  [for large W]
             = 3W²/8
```

Actually, more carefully:
```
Windows in cycle: W/2, W/2+1, ..., W
Number of RTTs: W/2 + 1

Average window = (W/2 + W)/2 = 3W/4

Packets sent ≈ (W/2) RTTs × (3W/4) packets/RTT
             = 3W²/8
```

Plus the W packets sent when loss occurs:
```
Total ≈ 3W²/8 + 3W/4
```

**Loss occurs once per cycle:**
```
Loss rate L = 1 / (packets per cycle)
            = 1 / (3W²/8 + 3W/4)
```

**Verified! ✓**

**b. Derive throughput formula:**

**Throughput = packets per cycle / time per cycle**

Time per cycle ≈ (W/2) RTTs

```
Throughput = (3W²/8 + 3W/4) / (W/2 × RTT)
           ≈ (3W²/8) / (W × RTT / 2)  [for large W]
           = (3W²/8) × (2/W × RTT)
           = 3W / (4 × RTT)
```

From part (a): L = 1/(3W²/8 + 3W/4) ≈ 8/(3W²) for large W

```
W² ≈ 8/(3L)
W ≈ √(8/(3L)) = √(8/3) / √L ≈ 1.633 / √L
```

Substitute into throughput:
```
Throughput = 3W / (4 × RTT)
           = 3/(4 × RTT) × 1.633/√L
           ≈ 1.22 / (RTT × √L)
```

In terms of MSS:
```
Throughput ≈ 1.22 × MSS / (RTT × √L)
```

**Verified! ✓**

This is the famous TCP throughput formula.

---

## P48-P50. TCP Throughput Analysis

**Given:**
- Link rate R = 10 Mbps
- RTT = 150 ms
- MSS = 1,500 bytes = 12,000 bits
- TCP Reno in congestion avoidance

**P48. Maximum window size**

Maximum window before link is fully utilized:
```
W_max × MSS / RTT = R

W_max = R × RTT / MSS
      = (10 × 10⁶ bits/s) × (0.15 s) / (12,000 bits)
      = 1,500,000 / 12,000
      = 125 packets
```

**Answer: W_max = 125 packets**

**P49. Average window and throughput**

In Reno, window oscillates: W_max/2 → W_max

```
W_avg = (W_max/2 + W_max) / 2
      = 3W_max/4
      = 3 × 125 / 4
      = 93.75 packets
```

Average throughput:
```
Throughput = W_avg × MSS / RTT
           = 93.75 × 12,000 bits / 0.15 s
           = 1,125,000 / 0.15
           = 7.5 Mbps
```

**Answer: W_avg = 93.75 packets, Throughput = 7.5 Mbps**

**P50. Time to reach W_max again after loss**

After loss: cwnd = W_max/2 = 62.5 ≈ 63 packets

Additive increase: +1 packet per RTT

```
Time = (W_max - W_max/2) × RTT
     = W_max/2 × RTT
     = 62.5 × 0.15 s
     = 9.375 seconds
```

**Answer: ≈ 9.4 seconds**

**For 10 Gbps link:**

```
W_max = (10 × 10⁹ bits/s) × (0.15 s) / (12,000 bits)
      = 125,000 packets

Time to reach W_max = 125,000/2 × 0.15 s
                    = 62,500 × 0.15
                    = 9,375 seconds
                    ≈ 2.6 hours!
```

**This is way too slow!**

**Proposed fix:**

1. **Larger initial window:** Start with larger cwnd
2. **Faster increase:** Increase by more than 1 MSS per RTT
3. **High-speed TCP variants:**
   - **TCP CUBIC:** Faster growth with cubic function
   - **Compound TCP:** Combines loss-based and delay-based
   - **BBR:** Bandwidth-delay product based
4. **Increase MSS:** Use jumbo frames (9000 bytes)
5. **Multiple parallel connections:** But reduces fairness

**Best solution: Use modern variants like CUBIC or BBR** that are designed for high-bandwidth-delay networks.

---

## P51-P54. AIMD Variants

These problems analyze mathematical relationships in AIMD:

**P51. Window growth time and average throughput**

Given window grows from W/2 to W in time T:

```
Growth: W/2 → W
Increase: W/2 packets
Rate: (W/2) / T packets per time unit
Each RTT: +1 packet
Number of RTTs: W/2
Time: T = (W/2) × RTT

Average window = 3W/4
Average throughput = (3W/4) / RTT
```

Relationship:
```
T = (W/2) × RTT
RTT = 2T/W

Throughput = (3W/4) / RTT
           = (3W/4) / (2T/W)
           = 3W² / (8T)
```

**P52. Multiplicative increase variant**

If increase is multiplicative: cwnd = cwnd × (1 + α)

Starting from W/2:
```
After n RTTs: W = (W/2) × (1 + α)ⁿ

To reach W:
(1 + α)ⁿ = 2
n = log(2) / log(1 + α)

Time T = n × RTT = RTT × log(2) / log(1 + α)
```

Much faster than additive! But doesn't converge to fairness.

**P53. Two connections fairness**

With AIMD, two connections converge to equal bandwidth:

```
Start: (x₁, x₂) where x₁ ≠ x₂
After AI: (x₁ + α, x₂ + α) - moves toward fairness
After MD: (βx₁, βx₂) - maintains ratio
Net: converges to (C/2, C/2)
```

**P54. Synchronization effects**

If connections synchronized (experience loss together):
- All increase together
- All decrease together
- Maintain relative ratios longer
- Slower convergence to fairness

Randomized timeouts and desynchronization help fairness.

---

## P55. High-Speed TCP Loss Tolerance

**For 10 Gbps connection:**

Using TCP formula: Throughput = 1.22 × MSS / (RTT × √L)

```
10 × 10⁹ = 1.22 × (1500 × 8) / (RTT × √L)

Assume RTT = 100 ms = 0.1 s:

10 × 10⁹ = (1.22 × 12,000) / (0.1 × √L)
10 × 10⁹ × 0.1 × √L = 14,640
√L = 14,640 / (10⁹)
√L = 1.464 × 10⁻⁵
L = 2.14 × 10⁻¹⁰
```

**Answer: Loss rate must be < 2 × 10⁻¹⁰** (1 loss per 5 billion packets!)

**For 100 Gbps:**

```
L = (1.22 × MSS / (RTT × Throughput))²
  = (1.22 × 12,000 / (0.1 × 10¹¹))²
  = (14,640 / 10¹⁰)²
  = 2.14 × 10⁻¹²
```

**Answer: Loss rate < 2 × 10⁻¹² **

These are unrealistically low! Standard TCP cannot efficiently use high-speed links. Need TCP variants (CUBIC, Compound, BBR).

---

## P56. TCP Idle Behavior

**Question:** After TCP goes idle and resumes, should it reuse old cwnd/ssthresh or reset?

**Option 1: Reuse old cwnd/ssthresh**

**Pros:**
- Fast startup after idle
- Utilizes previous knowledge of network capacity
- Good for bursty applications (web, email)

**Cons:**
- Network conditions may have changed during idle
- Could cause congestion burst
- Unfair to other flows that started during idle period

**Option 2: Reset to initial values**

**Pros:**
- Conservative, avoids congestion
- Fair to other flows
- Reprobe network conditions

**Cons:**
- Slow start penalty even if network unchanged
- Poor for applications with idle periods
- Wastes time reprobing known capacity

**Recommendation: Hybrid approach**

**TCP's actual behavior (RFC 5681):**
- If idle time < RTO: keep cwnd (short idle, conditions likely unchanged)
- If idle time > RTO: reset cwnd to initial window, keep ssthresh

**Better approach (RFC 2861 - Congestion Window Validation):**
```
- Track time since last sent packet
- If idle_time > RTT: gradually reduce cwnd
- Decay rate: cwnd = cwnd / 2 every RTT of idle time
- Minimum: cwnd = initial_cwnd
- Keep ssthresh for faster recovery
```

**Modern recommendation:**
```
if (idle_time < RTT):
    keep cwnd, ssthresh  // very short idle
elif (idle_time < several RTTs):
    cwnd = max(cwnd/2, initial_cwnd)  // moderate idle
else:
    cwnd = initial_cwnd  // long idle
    keep ssthresh for faster ramp-up
```

This balances performance and fairness.

---

## P57. End-Point Authentication

**a. UDP spoofing: Client spoofs IP Y as X**

**Question:** Where will server send reply?

**Answer: To address X (the spoofed address)**

**Explanation:**
- Client at Y sends UDP packet with source IP = X (spoofed)
- Server receives packet, believes it's from X
- Server sends reply to IP address X
- **Reply goes to X, not Y!**
- Y (attacker) doesn't receive the reply

**Use case for attacker:**
- Denial of Service: flood server with spoofed packets
- Reflection attack: cause server to send traffic to X (victim)
- Amplification: if response > request

**b. TCP SYN spoof: Can server be certain client is at Y?**

**Answer: NO, not with just SYN**

**TCP three-way handshake:**
```
1. Client → Server: SYN (seq=x)
2. Server → Client: SYN-ACK (seq=y, ack=x+1)
3. Client → Server: ACK (seq=x+1, ack=y+1)
```

**Spoofing scenario:**
```
Attacker at address A spoofs SYN with source = Y:

1. A → Server: SYN (source=Y, seq=x)
2. Server → Y: SYN-ACK (seq=y, ack=x+1)  [goes to Y!]
3. For connection to complete, need: Y → Server: ACK

But real host Y will either:
   - Ignore SYN-ACK (wasn't expecting it)
   - Send RST (reset)
   - Attacker A never sees SYN-ACK
```

**Can attacker complete handshake?**

**NO, unless:**
1. Attacker is on-path and can sniff SYN-ACK
2. Attacker can predict sequence number y
3. Attacker controls or has compromised Y

**Server's certainty:**

After **completed** handshake (received ACK):
- **More confident** client is at Y
- Client must have received SYN-ACK (to know seq y)
- But still not 100% certain:
  - On-path attacker could sniff
  - Sequence number prediction (old vulnerability)

**Modern security:**
- SYN cookies: stateless, resist SYN flood
- But don't authenticate client location
- Need application-layer authentication (TLS, SSH)

**Conclusion:** TCP handshake provides better assurance than UDP, but not cryptographic certainty. Need higher-layer authentication.

---

## P58. TCP Slow Start Delay

**Given:**
- Client-Server over single link
- Link rate = R
- RTT = constant
- Object size = 15S (where S is MSS)
- Slow start: cwnd starts at S, doubles each RTT until loss/threshold

**Assume:**
- Initial cwnd = 1 MSS = S
- Doubles each RTT: 1, 2, 4, 8, 16, ...
- No loss, ssthresh > 15

**Timeline:**

**Connection setup:** 1 RTT (SYN, SYN-ACK, ACK)

**Data transfer:**

**Condition a: 4S/R < 7S/R + RTT < 72S/R**

This seems unusual - let me interpret properly. Let's analyze slow start behavior:

```
RTT 0 (setup): 1 RTT
RTT 1: Send 1S, receive ACK after RTT
RTT 2: Send 2S (cwnd=2), receive ACKs
RTT 3: Send 4S (cwnd=4), receive ACKs  
RTT 4: Send 8S (cwnd=8), receive ACKs
       Total sent: 15S (complete!)
```

**Total time:**
- Setup: 1 RTT
- Data RTTs: Need to send 15S total

**Slow start transmission:**
```
Round 1: 1S sent, takes RTT + S/R
Round 2: 2S sent, takes RTT + 2S/R
Round 3: 4S sent, takes RTT + 4S/R
Round 4: 8S sent (only need 8S to complete 15S), takes 8S/R + RTT
```

**Total time = 1 RTT (setup) + RTT + S/R + RTT + 2S/R + RTT + 4S/R + 8S/R**
           **= 5 RTT + 15S/R**

For condition (a): This may specify when slow start completes in specific number of rounds.

**Condition b: S/R + RTT < 74S/R**

Simplifies to: RTT < 73S/R

This means RTT is small relative to transmission time.

**General answer:**
```
Total retrieval time = (1 + k) RTT + 15S/R
```
Where k = number of rounds in slow start to send 15S

Since 1 + 2 + 4 + 8 = 15 exactly:
**k = 4 rounds**

**Answer: Total time = 5 RTT + 15S/R**

**Condition c: S/R < RTT**

Transmission time per packet < propagation delay
Network is propagation-limited, not bandwidth-limited

Same formula applies: **5 RTT + 15S/R**

---

## Summary: Additional Problems (P2)

**P2. Figure 3.5 Analysis**

Without seeing Figure 3.5, typical scenario:
- Server has multiple clients
- Each client process has unique port
- Server uses well-known port (e.g., 80)

**Segments from server to clients:**
- Source port: Server's well-known port (80)
- Destination ports: Each client's ephemeral port (unique per client)

**IP addresses:**
- Source IP: Server's IP
- Destination IPs: Each client's IP

Example:
```
Client A (IP: 1.2.3.4, port: 5001) 
Client B (IP: 1.2.3.5, port: 5002)
Server S (IP: 10.0.0.1, port: 80)

S → A: src=(10.0.0.1:80), dst=(1.2.3.4:5001)
S → B: src=(10.0.0.1:80), dst=(1.2.3.5:5002)
```

---

## Conclusion

This completes the comprehensive solution set for all transport layer and reliable data transfer problems. The solutions cover:

✅ Port numbers and multiplexing
✅ Checksums and error detection  
✅ Reliable data transfer protocols (rdt1.0 - rdt3.0)
✅ Go-Back-N and Selective Repeat
✅ TCP segment structure and sequence numbers
✅ Flow control and congestion control
✅ TCP variants (Reno, CUBIC)
✅ Performance analysis and throughput calculations
✅ Security considerations (SYN cookies, spoofing)

Each solution includes detailed explanations, mathematical derivations, diagrams, and practical examples to ensure complete understanding of networking concepts at the transport layer.# Transport Layer & Reliable Data Transfer - Complete Solutions

## P1. Telnet Session Port Numbers

**Setup:** Two clients (A and B) initiate Telnet sessions with Server S.

**Solution:**

a. **Segments from A to S:**
   - Source port: Ephemeral port (e.g., 1234)
   - Destination port: 23 (Telnet server port)

b. **Segments from B to S:**
   - Source port: Different ephemeral port (e.g., 1235)
   - Destination port: 23

c. **Segments from S to A:**
   - Source port: 23
   - Destination port: 1234 (A's ephemeral port)

d. **Segments from S to B:**
   - Source port: 23
   - Destination port: 1235 (B's ephemeral port)

e. **Different hosts - same source port?**
   - Yes, it's possible. Different hosts can use the same ephemeral port number since they have different IP addresses. The socket is identified by the 4-tuple: (source IP, source port, dest IP, dest port).

f. **Same host - same source port?**
   - No, not possible. On the same host, each socket must have a unique port number. The OS will assign different ephemeral ports to A and B.

---

## P3. UDP and TCP Checksum

**Given bytes:** `01010011`, `01100110`, `01110100`

**Step 1: Sum the bytes**
```
  01010011
+ 01100110
-----------
  10111001
+ 01110100
-----------
 100101101 (9 bits - overflow!)
```

**Step 2: Handle wraparound**
```
  00101101  (lower 8 bits)
+        1  (carry bit)
-----------
  00101110
```

**Step 3: Take 1's complement**
```
1's complement of 00101110 = 11010001
```

**Answer: `11010001`**

**Why use 1's complement instead of just the sum?**
- Using just the sum would require sending additional information about what the correct sum should be
- With 1's complement, the receiver can verify correctness by adding all bytes (including checksum) - the result should be all 1s
- Provides error detection capability in a compact form

**How does receiver detect errors?**
- Receiver adds all bytes including the checksum
- Takes 1's complement of the result
- If result is all 0s (or equivalently, if sum before complement is all 1s), no error detected
- If result is anything else, error detected

**Can 1-bit error go undetected?**
- No, a single bit error will always be detected. Flipping any bit changes the sum.

**Can 2-bit error go undetected?**
- Yes, it's possible. If two bits flip in a way that their effects cancel out in the sum, the error won't be detected. For example, if one bit flips from 0→1 in position i and another flips from 1→0 in the same position i of a different byte, the sum remains unchanged.

---

## P4. 1's Complement Practice

**a. Bytes: `01011100` and `01100101`**

```
  01011100
+ 01100101
-----------
  11000001

1's complement = 00111110
```

**Answer: `00111110`**

**b. Bytes: `11011010` and `01100101`**

```
  11011010
+ 01100101
-----------
 100111111 (overflow)
  
  00111111  (lower 8 bits)
+        1  (carry)
-----------
  01000000

1's complement = 10111111
```

**Answer: `10111111`**

**c. Example where one bit flipped in each byte but checksum unchanged:**

Original bytes: `01011100` and `01100101` → checksum = `00111110`

Flip bit 0 of first byte: `01011100` → `01011101` (+1)
Flip bit 0 of second byte: `01100101` → `01100100` (-1)

New bytes: `01011101` and `01100100`

```
  01011101
+ 01100100
-----------
  11000001

1's complement = 00111110 (same!)
```

The effects cancel out because we added 1 to one byte and subtracted 1 from the other.

---

## P5. UDP Checksum Certainty

**Answer: No, the receiver cannot be absolutely certain.**

**Reasons:**
1. **Multiple bit errors can cancel out:** If bits flip in complementary positions across different bytes, the sum may remain unchanged
2. **All-zeros undetected:** If all bits in a word flip from their correct values to their complements in just the right way, the checksum can still match
3. **Limited detection:** The 16-bit checksum has only 2^16 possible values, so with enough bit errors, collisions are possible

**Example:** As shown in P4(c), flipping corresponding bits in opposite directions in different bytes can leave the checksum unchanged.

The checksum provides good error detection for random errors but cannot guarantee 100% detection, especially for burst errors or carefully crafted error patterns.

---

## P6. rdt2.1 Deadlock Scenario

**Deadlock Scenario:**

Consider rdt2.1 where:
- Sender sends packet with seq=0
- Receiver receives correctly, sends ACK0
- **ACK0 is corrupted in transit**
- Sender receives corrupted ACK, cannot tell if it's ACK0 or ACK1
- Sender retransmits packet with seq=0
- **This retransmission is lost**
- Sender waits for ACK (timeout not implemented in rdt2.1)
- Receiver waits for packet with seq=1

**Result: DEADLOCK**
- Sender is waiting for ACK for seq=0
- Receiver is waiting for data with seq=1
- Neither will proceed without timeout mechanism

This is why rdt2.1 is incomplete - it handles corruption but not loss. The protocol needs timeout and retransmission (as in rdt3.0) to avoid this deadlock.

---

## P8. FSM for rdt3.0 Receiver

```
┌─────────────────────────────────────────────────┐
│                                                 │
│         Wait for 0 from below                   │
│                                                 │
└─────────────────────────────────────────────────┘
                    │
                    │ rdt_rcv(rcvpkt) && 
                    │ notcorrupt(rcvpkt) && 
                    │ has_seq0(rcvpkt)
                    │
                    │ extract(rcvpkt, data)
                    │ deliver_data(data)
                    │ sndpkt = make_pkt(ACK, 0, checksum)
                    │ udt_send(sndpkt)
                    ↓
┌─────────────────────────────────────────────────┐
│                                                 │
│         Wait for 1 from below                   │
│                                                 │
└─────────────────────────────────────────────────┘
     ↑                              │
     │                              │ rdt_rcv(rcvpkt) && 
     │                              │ notcorrupt(rcvpkt) && 
     │                              │ has_seq1(rcvpkt)
     │                              │
     │                              │ extract(rcvpkt, data)
     │                              │ deliver_data(data)
     │                              │ sndpkt = make_pkt(ACK, 1, checksum)
     │                              │ udt_send(sndpkt)
     │                              ↓
     └──────────────────────────────

Self-transitions (not shown for clarity):
- In "Wait for 0": if corrupt(rcvpkt) || has_seq1(rcvpkt)
  → sndpkt = make_pkt(ACK, 1, checksum); udt_send(sndpkt)
  
- In "Wait for 1": if corrupt(rcvpkt) || has_seq0(rcvpkt)
  → sndpkt = make_pkt(ACK, 0, checksum); udt_send(sndpkt)
```

**Key Features:**
- Two states alternating between waiting for seq 0 and seq 1
- Extracts and delivers data only when correct, uncorrupted packet arrives
- Sends appropriate ACK for current expected sequence number
- Resends last ACK if wrong sequence number or corrupted packet arrives

---

## P9. rdt3.0 Trace with Garbled Packets

```
Sender                                  Receiver
  │                                        │
  │ rdt_send(data0)                        │
  │ [PKT0, seq=0]                          │
  ├────────────XXX (garbled)──────────────>│
  │                                        │ corrupt packet received
  │                                        │ (no ACK sent or 
  │                                        │  wrong ACK sent)
  │                                        │
  │ timeout                                │
  │ [PKT0, seq=0] (retransmit)             │
  ├───────────────────────────────────────>│
  │                                        │ rdt_rcv(PKT0)
  │                                        │ deliver_data(data0)
  │                                        │ send ACK0
  │<────────────────────────────────────XXX (garbled)
  │ corrupt ACK received                   │
  │                                        │
  │ timeout                                │
  │ [PKT0, seq=0] (retransmit)             │
  ├───────────────────────────────────────>│
  │                                        │ duplicate seq=0
  │                                        │ (discard, resend ACK0)
  │                                        │ send ACK0
  │<───────────────────────────────────────┤
  │ receive ACK0                           │
  │                                        │
  │ rdt_send(data1)                        │
  │ [PKT1, seq=1]                          │
  ├───────────────────────────────────────>│
  │                                        │ rdt_rcv(PKT1)
  │                                        │ deliver_data(data1)
  │                                        │ send ACK1
  │<───────────────────────────────────────┤
  │ receive ACK1                           │
  │                                        │
```

**Key Points:**
- Garbled data packet causes timeout and retransmission
- Garbled ACK also causes timeout and retransmission
- Receiver discards duplicate packets but resends ACK
- Sequence numbers (0,1) prevent duplicate delivery

---

## P10. Modified rdt2.1 with Timeout

**Modified rdt2.1 Protocol:**

**Sender FSM Changes:**
```
Wait for call 0 from above
  │
  │ rdt_send(data)
  │ sndpkt = make_pkt(0, data, checksum)
  │ udt_send(sndpkt)
  │ start_timer
  ↓
Wait for ACK0
  │
  ├─→ rdt_rcv(rcvpkt) && notcorrupt(rcvpkt) && isACK(rcvpkt, 0)
  │   stop_timer → [transition to Wait for call 1]
  │
  ├─→ rdt_rcv(rcvpkt) && (corrupt(rcvpkt) || isACK(rcvpkt, 1))
  │   [do nothing, wait for timeout]
  │
  └─→ timeout
      udt_send(sndpkt)
      start_timer
      [stay in same state]
```

(Similar for sequence number 1)

**Why it works correctly:**

1. **No packet loss beyond max delay:** Since channel guarantees maximum delay, if ACK doesn't arrive within timeout period, we know packet was lost

2. **Timeout value:** Set timeout > max_delay to ensure we don't timeout prematurely

3. **Handles corruption:** NAK/corruption handled by retransmission (already in rdt2.1)

4. **Handles loss:** New timeout mechanism handles lost packets by retransmitting

5. **Sequence numbers prevent duplicates:** Alternating 0/1 prevents receiver from accepting duplicate packets

6. **Correctness argument:**
   - If packet arrives: receiver sends ACK, sender gets it within max_delay
   - If packet lost: timeout fires after max_delay, sender retransmits
   - If ACK lost: timeout fires, sender retransmits, receiver recognizes duplicate
   - Eventually, packet and ACK both get through uncorrupted

---

## P11. rdt2.2 Self-Transition Analysis

**Protocol Context:** In rdt2.2 receiver, self-transitions occur when receiving duplicate packets.

**Question 1: Remove action from Wait-for-1-from-below state**

```
Current: rdt_rcv(rcvpkt) && notcorrupt(rcvpkt) && has_seq0(rcvpkt)
         sndpkt = make_pkt(ACK, 0, checksum)
         udt_send(sndpkt)
```

**If removed:** Protocol would **NOT work correctly**.

**Reason:** 
- Suppose receiver is waiting for seq=1, but receives duplicate seq=0
- This means sender didn't receive ACK0 and retransmitted
- If receiver doesn't resend ACK0, sender will keep retransmitting forever
- Communication halts

**Question 2: Remove action from Wait-for-0-from-below state**

**If removed:** Protocol would **NOT work correctly**.

**Scenario (as hinted):**
1. Sender sends packet with seq=0
2. **Packet is corrupted in transit**
3. Receiver is in Wait-for-0 state, receives corrupted packet
4. Receiver does nothing (action removed from self-transition)
5. Sender times out, retransmits seq=0
6. Receiver still in Wait-for-0, but now receives duplicate uncorrupted seq=0
7. **Without the resend action, sender never gets ACK0**
8. Deadlock - sender keeps timing out and retransmitting

**Conclusion:** Both self-transition ACK actions are essential for the protocol to work correctly. They ensure that if a sender retransmits due to lost/corrupted ACK, the receiver will resend the ACK.

---

## P12. rdt3.0 Retransmit on Wrong ACK

**Question:** What if rdt3.0 retransmits on corrupt/wrong ACK instead of ignoring?

**Analysis with bit errors only (no loss, but premature timeouts):**

Consider sending nth packet in the limit as n→∞:

**Original rdt3.0 (ignore wrong ACK):**
- Send packet n with seq bit b
- If premature timeout: resend once, eventually get correct ACK
- Packet n sent: 2 times (worst case with one premature timeout)

**Modified rdt3.0 (retransmit on wrong ACK):**
- Send packet n with seq bit b
- Premature timeout → retransmit → get ACK for packet (n-1) with seq bit !b
- Wrong ACK → retransmit → premature timeout → retransmit → get old ACK again
- This can **loop indefinitely**

**Trace Example:**
```
Time  | Event
------|--------------------------------------------------
t0    | Send PKT(n, seq=0)
t1    | Premature timeout
t2    | Retransmit PKT(n, seq=0)
t3    | Receive ACK(seq=1) [for previous packet]
t4    | Wrong ACK → Retransmit PKT(n, seq=0)
t5    | Premature timeout again
t6    | Retransmit PKT(n, seq=0)
t7    | Receive ACK(seq=1) again [still in network]
t8    | Loop continues...
```

**Answer:** The protocol would **NOT work correctly**. With premature timeouts, old ACKs can circulate in the network. Retransmitting on wrong ACK can cause infinite loops, where the sender repeatedly retransmits due to receiving old ACKs, which in turn triggers more timeouts. In the limit, packet n could be sent **infinite times**.

The original design (ignore wrong ACK) is correct because it breaks this cycle by only retransmitting on timeout.

---

## P13. Alternating-Bit Protocol with Reordering

**Diagram showing protocol failure with message reordering:**

```
Sender                              Receiver
(seq=0)                            (expecting 0)
  │                                      │
  │ D0 (data, seq=0) ────┐               │
  │                      │               │
  │ D1 (data, seq=1) ──┐ │               │
  │                    │ └──────────────>│ Receive D0
  │                    │                 │ Deliver data0
  │                    │                 │ Send A0
  │                    │                 │ (now expecting 1)
  │                    │  ┌──────────────┤ A0
  │<───────────────────┼──┘              │
  │ Receive A0         │                 │
  │ (move to seq=1)    │                 │
  │                    │                 │
  │                    └────────────────>│ Receive D1
  │                                      │ Deliver data1
  │                                      │ Send A1
  │                                      │ (now expecting 0)
  │                     ┌────────────────┤ A1
  │<────────────────────┘                │
  │ Receive A1                           │
  │ (move to seq=0)                      │
  │                                      │
  │ D2 (data, seq=0) ────┐               │
  │                      │               │
  │ D3 (data, seq=1) ──┐ │               │
  │                    │ │               │
  │                    │ └──────────────>│ Receive D2
  │                    │                 │ Deliver data2
  │                    │                 │ Send A0
  │                    │                 │
  │                    └───┐             │
  │                        └────────────>│ Receive D3
  │                                      │ Deliver data3
  │                                      │ Send A1
  │                                      │
  
BUT NOW: If D1 was delayed and arrives after D3...
  │                                      │
  │ (Delayed D1) ───────────────────────>│ Receive D1 (seq=1)
  │                                      │ Currently expecting 0
  │                                      │ Think it's duplicate!
  │                                      │ DISCARD data1 (but it's really data1!)
  │                                      │ Resend A1
  │                                      │ OR
  │                                      │ If expecting 1:
  │                                      │ Accept as NEW data
  │                                      │ DELIVER DUPLICATE data1!
  │                                      │
```

**Problem - Sense in which it fails:**

1. **Duplicate Delivery:** If D1 is delayed until after D3 is received, and the receiver is expecting seq=1 again, D1 will be accepted as new data and delivered twice

2. **Data Loss:** Alternatively, if receiver is expecting seq=0 when delayed D1 arrives, it discards D1 thinking it's a duplicate, causing actual data loss

3. **Out-of-order delivery:** Even if we try to accept all packets, we can't distinguish between:
   - A delayed old packet with seq=1
   - A new packet with seq=1

**Conclusion:** The alternating-bit protocol **requires in-order delivery**. With only 2 sequence numbers (0,1), we cannot distinguish between delayed old packets and new packets after a full cycle. We need larger sequence number space to handle reordering (as in GBN/SR protocols).

---

## P14. ACK vs NAK Protocols

**Scenario 1: Sender sends data infrequently**

**NAK-only protocol preferable? YES**

**Reasoning:**
- With infrequent data, most of the time receiver is idle
- ACK protocol: Every packet requires ACK → network traffic = 2× (data + ACK)
- NAK protocol: Only send NAK when error detected → network traffic ≈ data only (assuming low error rate)
- If error rate is low, NAK-only generates much less control traffic
- Timeout at sender handles packet loss (no ACK means successful receipt)

**Scenario 2: Sender has lots of data, few losses**

**NAK-only protocol preferable? NO**

**Reasoning:**
- With high data rate and low loss rate, sender needs frequent confirmation
- NAK-only requires sender to timeout for every packet (to confirm receipt)
- Timeout must be conservative (long) to avoid false positives
- ACK protocol: Immediate feedback, allows pipelining, sender knows exactly which packets received
- With pipelining (GBN/SR), ACKs enable high throughput
- NAK-only becomes bottleneck: sender must wait for timeout on each packet before being confident
- ACK-based protocols (especially cumulative ACKs in GBN) efficiently handle bulk transfers

**Summary:**
- **Infrequent sending + low error rate:** NAK-only is better (less overhead)
- **High data rate + few losses:** ACK-based is better (enables pipelining and fast feedback)

---

## P15. Channel Utilization and Window Size

**Given:** Cross-country example (Figure 3.17 reference)
- Packet size: 1,500 bytes = 12,000 bits
- Want utilization > 98%

**Need to determine from typical scenario:**
- Let's assume RTT = 30 ms (typical cross-country USA)
- Assume link rate R = 1 Gbps (modern network)

**Utilization formula for pipelining:**

U = (W × L) / (RTT × R + L)

Where:
- W = window size (packets)
- L = packet size (bits)
- RTT = round-trip time
- R = link rate (bps)

**For utilization > 0.98:**

U = (W × L) / (RTT × R) ≈ 0.98 (when RTT × R >> L)

W > 0.98 × (RTT × R) / L

**Calculation:**

W > 0.98 × (0.030 s × 1×10^9 bps) / (12,000 bits)
W > 0.98 × (30,000,000) / 12,000
W > 0.98 × 2,500
W > 2,450 packets

**Answer: Window size must be at least 2,450 packets** (for assumed parameters).

**General formula:**

W > 0.98 × RTT × R / L

For any specific scenario, substitute actual RTT and R values.

---

## P16. rdt3.0 with Multiple Duplicate ACKs

**Design:** Receiver sends multiple duplicate ACKs even before corresponding data arrives.

**Would this increase channel utilization? NO**

**Analysis:**

1. **Stop-and-wait constraint remains:** rdt3.0 sender waits for correct ACK before sending next packet. Receiving additional duplicate ACKs doesn't help because:
   - Sender is waiting for ACK for packet n
   - Receiver sending duplicate ACK for packet (n-1) doesn't satisfy sender's wait condition
   - Sender still cannot send packet (n+1) until ACK for packet n arrives

2. **Utilization formula:** U = L/(RTT × R + L)
   - This is determined by round-trip time, not ACK frequency
   - Multiple ACKs don't reduce RTT

**Potential Problems:**

1. **Increased network congestion:** Multiple duplicate ACKs add unnecessary traffic, wasting bandwidth

2. **Confusion at sender:** Sender might misinterpret multiple ACKs as corruption or protocol errors

3. **ACK processing overhead:** Sender must process and discard all extra ACKs

4. **Buffer waste:** Extra ACK packets consume buffer space at routers

5. **Wrong optimization target:** Problem is stop-and-wait, not ACK reliability. Solution should be pipelining (GBN or SR), not more ACKs

**Correct approach to improve utilization:** Use pipelining protocols (Go-Back-N or Selective Repeat) instead of modifying rdt3.0.

---

## P17. Alternating Sender-Receiver FSM

**Entity A (starts as sender):**

```
┌─────────────────────────────────────┐
│  Initial State: Ready to Send       │<─┐
└─────────────────────────────────────┘  │
    │                                    │
    │ rdt_send(data)                     │
    │ packet = make_pkt(data)            │
    │ udt_send(packet)                   │
    ↓                                    │
┌─────────────────────────────────────┐  │
│  Waiting for Delivery Confirmation   │  │
└─────────────────────────────────────┘  │
    │                                    │
    │ rdt_rcv(packet)                    │
    │ extract(packet, data)              │
    │ deliver_data(data)                 │
    └────────────────────────────────────┘

Self-transition on "Ready to Send":
  rdt_send(data) → rdt_unable_to_send(data)
  (when receiving send request while waiting to receive)
```

**Entity B (starts as receiver):**

```
┌─────────────────────────────────────┐
│  Initial State: Ready to Receive    │<─┐
└─────────────────────────────────────┘  │
    │                                    │
    │ rdt_rcv(packet)                    │
    │ extract(packet, data)              │
    │ deliver_data(data)                 │
    ↓                                    │
┌─────────────────────────────────────┐  │
│  Ready to Send                       │  │
└─────────────────────────────────────┘  │
    │                                    │
    │ rdt_send(data)                     │
    │ packet = make_pkt(data)            │
    │ udt_send(packet)                   │
    └────────────────────────────────────┘

Self-transition on "Ready to Receive":
  rdt_send(data) → rdt_unable_to_send(data)
  (when receiving send request while waiting to receive)
```

**Flow:**
1. A starts in "Ready to Send", B in "Ready to Receive"
2. A sends → transitions to "Waiting for Delivery Confirmation"
3. B receives → delivers → transitions to "Ready to Send"
4. B sends → transitions to "Ready to Receive"
5. A receives → delivers → transitions to "Ready to Send"
6. Cycle repeats: A→B→A→B...

---

## P18. Selective Repeat - Send Two at a Time

**Design: SR Protocol with Paired Transmission**

**Packet Format:**
```
┌─────────┬─────────┬────────┬──────────┐
│ Seq Num │ Pair ID │  Data  │ Checksum │
└─────────┴─────────┴────────┴──────────┘
```
- Seq Num: Unique sequence number (0, 1, 2, 3, ...)
- Pair ID: 0 or 1 (which packet in the pair)
- Data: Payload
- Checksum: Error detection

**Sender FSM:**

```
┌──────────────────────────────┐
│  Wait for Data (Pair Empty)  │
└──────────────────────────────┘
    │ rdt_send(data0)
    │ Store in pair_buffer[0]
    ↓
┌──────────────────────────────┐
│  Wait for Second Packet      │
└──────────────────────────────┘
    │ rdt_send(data1)
    │ Store in pair_buffer[1]
    │ packet0 = make_pkt(seq_base, 0, data0, checksum)
    │ packet1 = make_pkt(seq_base+1, 1, data1, checksum)
    │ udt_send(packet0)
    │ udt_send(packet1)
    │ start_timer(seq_base)
    │ start_timer(seq_base+1)
    ↓
┌──────────────────────────────┐
│  Wait for Both ACKs          │
└──────────────────────────────┘
    │
    ├─→ rdt_rcv(ACK, seq_base) 
    │   ack_received[0] = true
    │   stop_timer(seq_base)
    │
    ├─→ rdt_rcv(ACK, seq_base+1)
    │   ack_received[1] = true
    │   stop_timer(seq_base+1)
    │
    ├─→ timeout(seq_base) && !ack_received[0]
    │   udt_send(packet0)
    │   start_timer(seq_base)
    │
    ├─→ timeout(seq_base+1) && !ack_received[1]
    │   udt_send(packet1)
    │   start_timer(seq_base+1)
    │
    └─→ ack_received[0] && ack_received[1]
        seq_base += 2
        → Return to "Wait for Data"
```

**Receiver FSM:**

```
┌──────────────────────────────┐
│  Wait for Packets            │
│  (expecting seq_base,        │
│   seq_base+1)                │
└──────────────────────────────┘
    │
    ├─→ rdt_rcv(packet) && has_seq(packet, seq_base)
    │   extract(packet, data)
    │   buffer[seq_base] = data
    │   send ACK(seq_base)
    │
    ├─→ rdt_rcv(packet) && has_seq(packet, seq_base+1)
    │   extract(packet, data)
    │   buffer[seq_base+1] = data
    │   send ACK(seq_base+1)
    │
    └─→ buffer[seq_base] != null && buffer[seq_base+1] != null
        deliver_data(buffer[seq_base])
        deliver_data(buffer[seq_base+1])
        seq_base += 2
        clear buffer

Self-transition for duplicate/out-of-range:
    rdt_rcv(packet) && (seq < seq_base || seq > seq_base+1)
    → send ACK(seq)  [acknowledge old packets]
```

**Timeline Trace (Packet 1 Lost):**

```
Time  Sender                    Network              Receiver
─────────────────────────────────────────────────────────────
t0    Send PKT0(seq=0, pair=0)
      Send PKT1(seq=1, pair=1)
                                PKT0 ──────────────> Receive PKT0
                                                     Buffer[0] = data0
                                                     Send ACK(0)
                                PKT1 ────X (lost)
                                
t1                              <────────────── ACK0 Receive ACK(0)
                                                     ack_rcv[0] = true
                                                     
t2    Timeout for seq=1
      Retransmit PKT1
                                PKT1 ──────────────> Receive PKT1
                                                     Buffer[1] = data1
                                                     Send ACK(1)
                                                     Both buffered!
                                                     Deliver data0, data1
                                                     
t3                              <────────────── ACK1 Receive ACK(1)
                                                     ack_rcv[1] = true
                                                     Both ACKs received
                                                     seq_base += 2
      Both ACKs received                             Ready for next pair
      Ready to send next pair
```

---

## P19. Reliable Broadcast: A → B & C

**Scenario:** Host A broadcasts to B and C; must wait for both ACKs before sending next packet.

**Packet Format:**
```
┌─────────┬────────┬──────────┐
│ Seq Num │  Data  │ Checksum │
└─────────┴────────┴──────────┘
```

**ACK Format:**
```
┌─────────┬─────────┬──────────┐
│ Seq Num │ Host ID │ Checksum │
└─────────┴─────────┴──────────┘
```

**Host A FSM (Sender):**

```
┌──────────────────────────────────────┐
│  Wait for Data (seq = 0 initially)   │
└──────────────────────────────────────┘
    │ rdt_send(data)
    │ sndpkt = make_pkt(seq, data, checksum)
    │ udt_send(sndpkt)  [broadcast to B and C]
    │ start_timer
    │ ack_B = false
    │ ack_C = false
    ↓
┌──────────────────────────────────────┐
│  Wait for ACKs from B and C          │
└──────────────────────────────────────┘
    │
    ├─→ rdt_rcv(ACK) && from_B && has_seq(ACK, seq)
    │   ack_B = true
    │   if (ack_B && ack_C):
    │       stop_timer
    │       seq = 1 - seq  [toggle]
    │       → goto "Wait for Data"
    │
    ├─→ rdt_rcv(ACK) && from_C && has_seq(ACK, seq)
    │   ack_C = true
    │   if (ack_B && ack_C):
    │       stop_timer
    │       seq = 1 - seq  [toggle]
    │       → goto "Wait for Data"
    │
    └─→ timeout
        udt_send(sndpkt)  [rebroadcast]
        start_timer
        [stay in same state]
```

**Host C FSM (Receiver - same for B):**

```
┌──────────────────────────────────────┐
│  Wait for seq from A                 │
│  (expecting_seq = 0 initially)       │
└──────────────────────────────────────┘
    │
    ├─→ rdt_rcv(pkt) && notcorrupt(pkt) && 
    │   has_seq(pkt, expecting_seq)
    │   extract(pkt, data)
    │   deliver_data(data)
    │   sndpkt = make_pkt(ACK, expecting_seq, myID, checksum)
    │   udt_send(sndpkt)  [to A]
    │   expecting_seq = 1 - expecting_seq  [toggle]
    │
    └─→ rdt_rcv(pkt) && (corrupt(pkt) || 
        has_seq(pkt, 1-expecting_seq))
        [received duplicate or corrupt]
        sndpkt = make_pkt(ACK, 1-expecting_seq, myID, checksum)
        udt_send(sndpkt)  [resend last ACK]
```

**Key Features:**
- A waits for ACKs from BOTH B and C before proceeding
- Sequence number alternates (0,1) to detect duplicates
- Timeout and retransmission handles loss
- Receivers send ACK for current expected packet
- Receivers resend last ACK if duplicate received

---

## P20. Alternating Delivery at Receiver C

**Scenario:** A and C communicate, B and C communicate. C must deliver alternately: A→B→A→B...

**Packet Format:**
```
┌──────────┬─────────┬────────┬──────────┐
│ Sender   │ Seq Num │  Data  │ Checksum │
│ ID (A/B) │  (0/1)  │        │          │
└──────────┴─────────┴────────┴──────────┘
```

**Host A FSM (Sender):**

```
┌──────────────────────────────────────┐
│  Wait for Data (seq_A = 0)           │
└──────────────────────────────────────┘
    │ rdt_send(data)
    │ sndpkt = make_pkt(A, seq_A, data, checksum)
    │ udt_send(sndpkt)  [to C]
    │ start_timer
    ↓
┌──────────────────────────────────────┐
│  Wait for ACK from C                 │
└──────────────────────────────────────┘
    │
    ├─→ rdt_rcv(ACK) && notcorrupt(ACK) && 
    │   has_seq(ACK, seq_A) && from_A
    │   stop_timer
    │   seq_A = 1 - seq_A  [toggle]
    │   → goto "Wait for Data"
    │
    └─→ timeout
        udt_send(sndpkt)  [retransmit]
        start_timer
```

(Host B FSM is identical to A, with its own seq_B)

**Host C FSM (Receiver with Alternating Delivery):**

```
┌──────────────────────────────────────┐
│  Wait for Data from A                │
│  (expect_A = 0, expect_B = 0)        │
└──────────────────────────────────────┘
    │
    ├─→ rdt_rcv(pkt) && notcorrupt(pkt) && 
    │   from_A && has_seq(pkt, expect_A)
    │   extract(pkt, data)
    │   deliver_data(data)
    │   sndpkt = make_pkt(ACK, expect_A, A, checksum)
    │   udt_send(sndpkt)  [to A]
    │   expect_A = 1 - expect_A  [toggle]
    │   → goto "Wait for Data from B"
    │
    ├─→ rdt_rcv(pkt) && from_A && 
    │   (corrupt(pkt) || has_seq(pkt, 1-expect_A))
    │   [duplicate or corrupt from A]
    │   sndpkt = make_pkt(ACK, 1-expect_A, A, checksum)
    │   udt_send(sndpkt)  [resend last ACK to A]
    │   [stay in same state - still waiting for A]
    │
    └─→ rdt_rcv(pkt) && from_B
        [ignore B's packet - not B's turn]
        [optionally send negative response]
        [stay in same state]

┌──────────────────────────────────────┐
│  Wait for Data from B                │
└──────────────────────────────────────┘
    │
    ├─→ rdt_rcv(pkt) && notcorrupt(pkt) && 
    │   from_B && has_seq(pkt, expect_B)
    │   extract(pkt, data)
    │   deliver_data(data)
    │   sndpkt = make_pkt(ACK, expect_B, B, checksum)
    │   udt_send(sndpkt)  [to B]
    │   expect_B = 1 - expect_B  [toggle]
    │   → goto "Wait for Data from A"
    │
    ├─→ rdt_rcv(pkt) && from_B && 
    │   (corrupt(pkt) || has_seq(pkt, 1-expect_B))
    │   [duplicate or corrupt from B]
    │   sndpkt = make_pkt(ACK, 1-expect_B, B, checksum)
    │   udt_send(sndpkt)  [resend last ACK to B]
    │   [stay in same state - still waiting for B]
    │
    └─→ rdt_rcv(pkt) && from_A
        [ignore A's packet - not A's turn]
        [stay in same state]
```

**Key Features:**
- C has two states: waiting for A, waiting for B
- State transitions enforce alternating delivery
- Each sender (A, B) has independent sequence numbers
- C ignores packets from the "wrong" sender
- Duplicate detection via sequence numbers prevents data loss

---

## P21. Request-Data Protocol

**Scenario:** 
- A sends R (Request) messages → can be lost
- B replies with D (Data) messages → always delivered correctly
- A must deliver exactly one copy of each D

**Packet Formats:**
```
Request:  ┌─────────┬──────────┐
          │ Req Num │ Checksum │
          └─────────┴──────────┘

Data:     ┌─────────┬────────┬──────────┐
          │ Req Num │  Data  │ Checksum │
          └─────────┴────────┴──────────┘
```

**Host A FSM (Requester):**

```
┌──────────────────────────────────────┐
│  Ready to Request (req_num = 0)      │
└──────────────────────────────────────┘
    │ rdt_send(request)  [from upper layer]
    │ sndpkt = make_pkt(R, req_num)
    │ udt_send(sndpkt)  [to B]
    │ start_timer
    ↓
┌──────────────────────────────────────┐
│  Wait for Data                       │
└──────────────────────────────────────┘
    │
    ├─→ rdt_rcv(pkt) && is_data(pkt) && 
    │   has_req_num(pkt, req_num)
    │   extract(pkt, data)
    │   deliver_data(data)
    │   stop_timer
    │   req_num++
    │   → goto "Ready to Request"
    │
    └─→ timeout
        udt_send(sndpkt)  [retransmit request]
        start_timer
        [stay in same state]
```

**Host B FSM (Data Provider):**

```
┌──────────────────────────────────────┐
│  Wait for Request                    │
│  (last_req = -1)                     │
└──────────────────────────────────────┘
    │
    └─→ rdt_rcv(pkt) && is_request(pkt) && 
        has_req_num(pkt, N)
        
        if (N > last_req):
            [New request]
            data = fetch_data()  [get data from upper layer]
            sndpkt = make_pkt(D, N, data, checksum)
            udt_send(sndpkt)  [to A - guaranteed delivery]
            last_req = N
        else:
            [Duplicate request - A didn't get our data]
            [Resend the data for req N]
            udt_send(cached_response[N])
```

**Key Features:**
- No sequence numbers needed for data (since D always delivered)
- Request number ensures A delivers exactly one copy
- A retransmits R if timeout (R can be lost)
- B detects duplicate R and resends corresponding D
- Simple: leverages guarantee that D is always delivered

**Why minimal:**
- No ACKs needed (D delivery is guaranteed)
- No sequence numbers on D (in-order guaranteed)
- Only request numbers to match R with D

---

## P22. GBN Protocol (Window = 4)

**Given:**
- Sender window size = 4
- Sequence number range = 1,024 (0 to 1,023)
- At time t, receiver expects sequence number k

**a. Possible sets of sequence numbers in sender's window:**

The sender's window contains 4 consecutive sequence numbers. The base of the window can be:
- At most k (if receiver expecting k, sender hasn't moved past k yet)
- At least k-3 (sender could be up to 3 packets ahead in sending)

But sender cannot be ahead of receiver by more than window size in GBN.

**Possible window sets:**
1. [k, k+1, k+2, k+3] - receiver just ACKed k-1
2. [k-1, k, k+1, k+2] - receiver waiting for k, sender sent up to k+2
3. [k-2, k-1, k, k+1] - receiver waiting for k, sender sent up to k+1
4. [k-3, k-2, k-1, k] - receiver waiting for k, sender just sent k

**Answer:** Any window [k-i, k-i+1, k-i+2, k-i+3] where i ∈ {0, 1, 2, 3}

(All arithmetic mod 1,024)

**b. Possible values of ACK field in ACK messages:**

In GBN, receiver sends cumulative ACK for last correctly received in-order packet.

If receiver is expecting k, it means:
- Receiver has received all packets up to k-1
- Last ACK sent was ACK(k-1)

But there could be ACK messages in transit that were sent earlier:

**Possible ACK values:**
- ACK(k-1) - most recent, saying "received up to k-1"
- ACK(k-2), ACK(k-3), ACK(k-4) - older ACKs still in network

Since window size is 4, sender's base could have been as far back as k-4, so ACKs from that time could still be propagating.

**Answer:** ACK field values in {k-4, k-3, k-2, k-1} (mod 1,024)

---

## P23. GBN vs SR Window Limits

**Given:** Sequence number space = k (i.e., sequence numbers from 0 to k-1)

**Go-Back-N (GBN):**

In GBN, sender can have window size N, but we need to avoid ambiguity where:
- Old packet with seq number x
- New packet with seq number x (after wraparound)

**Analysis:**
- Receiver expects seq number i
- Sender's window could contain up to [i, i+1, ..., i+N-1]
- After full cycle, seq i appears again after k packets
- To avoid confusion: i + N < i + k
- Therefore: N < k

But we also need to ensure that when sender wraps around, receiver has moved forward enough.

**Maximum window size for GBN: N ≤ k - 1**

But practically, to avoid any ambiguity even with lost ACKs:

**Answer: N ≤ k - 1** or more conservatively **N ≤ k/2**

The standard answer is: **N ≤ k - 1**

**Selective Repeat (SR):**

In SR, both sender and receiver have windows. More restrictive condition needed.

**Analysis:**
- Sender window size = N
- Receiver window size = N
- Total "in-flight" range = 2N
- After wraparound (k packets), we need no overlap

To avoid ambiguity between old and new packets:
- Sender could have packets [i to i+N-1]
- Receiver could have buffered [i to i+N-1]
- After wraparound, seq i appears at position i+k
- Need: i+k should not be in either window when i is still valid

**Condition:** 2N ≤ k

**Maximum window size for SR: N ≤ k/2**

**Summary:**
- **GBN:** N_max = k - 1
- **SR:** N_max = k/2

---

## P24. True/False

**a. With SR, sender can receive ACK for a packet outside current window.**

**TRUE**

**Justification:** 
In SR, sender maintains a window and advances it as ACKs arrive. If sender has window [5,6,7,8] and receives ACK for packet 4 (which was already ACKed and removed from window), this ACK is outside the current window. This can happen due to:
- Delayed ACK in network
- Duplicate ACK from receiver
The sender simply ignores such ACKs.

**b. With GBN, sender can receive ACK for a packet outside current window.**

**TRUE**

**Justification:**
Similar to SR, in GBN the sender can receive delayed or duplicate ACKs for packets that have already been acknowledged and removed from the window. For example:
- Sender window: [10,11,12,13]
- Receives ACK(9) - outside window (already processed)
- Or receives ACK(14) - if receiver sent cumulative ACK that jumped ahead

GBN sender typically uses cumulative ACKs, so ACK(n) acknowledges all packets up to n. An old ACK(8) when window is [10,11,12,13] is outside and ignored.

**c. Alternating-bit protocol = SR with sender/receiver window size 1.**

**TRUE**

**Justification:**
Alternating-bit protocol:
- Uses sequence numbers 0 and 1
- Sender sends one packet, waits for ACK
- Receiver expects one specific packet

SR with window size 1:
- Sender window = 1 packet
- Receiver window = 1 packet
- Sender waits for ACK before sending next
- Receiver accepts only expected packet

These are functionally identical. Both implement stop-and-wait with sequence number space of 2.

**d. Alternating-bit protocol = GBN with sender/receiver window size 1.**

**TRUE**

**Justification:**
GBN with window size 1:
- Sender sends one packet and waits
- Receiver accepts only in-order packets (cumulative ACK)
- With window = 1, no pipelining occurs

This is also equivalent to alternating-bit protocol. With window size 1, GBN reduces to stop-and-wait with the same behavior as alternating-bit.

Both (c) and (d) are true because SR and GBN behave identically when window size is 1.

---

## P25. UDP Application Control

**Why does UDP give more control over WHAT data is sent and WHEN?**

**UDP Characteristics:**

1. **No buffering/segmentation:** 
   - Application writes X bytes → UDP sends X bytes (+ header)
   - TCP may buffer, split, or combine data into segments based on MSS, Nagle's algorithm, etc.

2. **No transmission delay:**
   - UDP sends immediately when application calls send()
   - TCP may delay transmission due to:
     - Nagle's algorithm (wait for full MSS or ACK)
     - Congestion control (wait for cwnd to allow sending)
     - Flow control (wait for receiver window)

3. **No retransmission:**
   - UDP sends once; application controls retries
   - TCP automatically retransmits, application has no control over timing

4. **Message boundaries preserved:**
   - UDP maintains datagram boundaries
   - TCP is byte-stream; no message boundaries

5. **No rate control:**
   - Application can send at any rate
   - TCP throttles based on congestion/flow control

**Examples:**

**WHAT data:**
- VoIP app wants to send exactly 160 bytes every 20ms
- UDP: sends exactly 160 bytes
- TCP: might bundle multiple calls into larger segment

**WHEN sent:**
- Gaming app needs packet sent immediately
- UDP: sends now
- TCP: might delay due to Nagle's algorithm or congestion control

**Conclusion:** UDP is a thin wrapper over IP, giving applications direct control. TCP provides reliability and flow/congestion control, but takes control away from application regarding timing and segmentation.

---

## P26. TCP Sequence Number Exhaustion

**Given:** MSS = 536 bytes

**a. Maximum file size L without sequence number exhaustion:**

TCP sequence number field = 4 bytes = 32 bits
Maximum sequence numbers = 2^32 = 4,294,967,296

Each byte of data gets one sequence number.

**Maximum L = 2^32 bytes = 4,294,967,296 bytes**

In more readable units:
- L = 4 GB (exactly 4 × 2^30 bytes)
- L = 4,294,967,296 bytes

**b. Time to transmit L over 155 Mbps link:**

Transmission time = L / R (ignoring flow and congestion control)

T = (4,294,967,296 bytes × 8 bits/byte) / (155 × 10^6 bits/sec)

T = 34,359,738,368 bits / 155,000,000 bits/sec

T = 221.67 seconds

**T ≈ 222 seconds ≈ 3.7 minutes**

**Answers:**
- a. L_max = 4,294,967,296 bytes = 4 GB
- b. T ≈ 222 seconds

---

## P27. TCP Segment Details

**Setup:**
- Host A → Host B
- B has received up to byte 126 (expecting byte 127)
- A sends two segments back-to-back:
  - First: 80 bytes, seq=127, src=302, dst=80
  - Second: 40 bytes

**a. Second segment: seq, src port, dst port**

- **Sequence number:** 127 + 80 = 207
  (First segment contains bytes 127-206, second starts at 207)
- **Source port:** 302 (same connection)
- **Destination port:** 80 (same connection)

**Answer: seq=207, src=302, dst=80**

**b. ACK if first arrives before second:**

B receives first segment (bytes 127-206):
- Acknowledges: ACK = 207 (next expected byte)
- Source port: 80
- Destination port: 302

When second segment arrives (bytes 207-246):
- Acknowledges: ACK = 247
- Source port: 80
- Destination port: 302

**Answer: First ACK=207, then ACK=247 (both src=80, dst=302)**

**c. ACK if second arrives before first:**

B is expecting byte 127, but receives second segment (bytes 207-246):
- Out of order! B buffers it
- Sends duplicate ACK = 127 (still expecting 127)
- Source port: 80, Destination port: 302

When first segment arrives (bytes 127-206):
- Now has bytes 127-246 contiguous
- Acknowledges: ACK = 247
- Source port: 80, Destination port: 302

**Answer: Duplicate ACK=127, then ACK=247 (both src=80, dst=302)**

**d. Timing diagram:**

```
Time    Host A (302)                       Host B (80)
        Expecting ACK for sent data        Last received: byte 126
────────────────────────────────────────────────────────────────

t0      Send SEG1
        [seq=127, len=80]
        [bytes 127-206]
        ────────────────────────────────────>
        
t1      Send SEG2
        [seq=207, len=40]
        [bytes 207-246]
        ────────────────────────────────────>
        
t2                                         Receive SEG1
                                           [bytes 127-206]
                                           Send ACK=207
        <────────────────────────────────────
        
t3                                         Receive SEG2
                                           [bytes 207-246]
                                           Send ACK=247
        <────────────────────────────────────
        
t4      Receive ACK=207
        [acknowledges bytes up to 206]
        
t5      Receive ACK=247
        [acknowledges bytes up to 246]


Alternate scenario (SEG2 arrives first):

t0      Send SEG1 [seq=127, len=80] ──────>
t1      Send SEG2 [seq=207, len=40] ──┐
                                      │
t2                                    └───> Receive SEG2 (out of order!)
                                           Buffer it
                                           Send duplicate ACK=127
        <────────────────────────────────────
        
t3                                    ┌──> Receive SEG1
        (delayed) ────────────────────┘    [bytes 127-206]
                                           Now have 127-246!
                                           Send ACK=247
        <────────────────────────────────────
        
t4      Receive ACK=127 (duplicate)
t5      Receive ACK=247
```

---

## P28. TCP Flow Control Effect

**Setup:**
- Host A → Host B
- Link capacity: 100 Mbps
- A can send at: 120 Mbps
- B can read at: 50 Mbps
- Perfect TCP (no loss)

**Effect of TCP Flow Control:**

**Initial Phase:**
1. A starts sending at up to 120 Mbps (limited by congestion control)
2. B's receive buffer starts filling up
3. B reads at 50 Mbps, but receives at 100 Mbps (link capacity)
4. Buffer accumulates data at: 100 - 50 = 50 Mbps

**Flow Control Kicks In:**
5. B's receive buffer fills up
6. B advertises smaller receive window (rwnd) in ACKs
7. A must limit sending to: min(cwnd, rwnd)
8. rwnd decreases as buffer fills

**Steady State:**
9. A's effective sending rate throttled to match B's reading rate
10. **A sends at 50 Mbps** (matching B's consumption rate)
11. Network link underutilized: only 50 of 100 Mbps used
12. B's buffer oscillates between nearly empty and partially full

**Key Effects:**
- **Rate limiting:** A reduced from potential 120 Mbps to 50 Mbps
- **Buffer management:** B's buffer prevents overflow
- **Link underutilization:** 100 Mbps link only uses 50 Mbps
- **Application bottleneck:** B's slow reading becomes bottleneck

**Diagram:**
```
A's capability: 120 Mbps ───┐
                            │ TCP Flow Control
Link capacity: 100 Mbps ────┤ limits A to match
                            │ B's reading rate
B's reading: 50 Mbps ───────┘
                            
Actual throughput: 50 Mbps
```

Flow control ensures data delivery matches receiver's processing capability, preventing buffer overflow but potentially underutilizing network resources.

---

## P29. SYN Cookies

**Context:** SYN cookies defend against SYN flood attacks by avoiding state allocation until connection established.

**a. Why must server use special ISN in SYNACK?**

**Answer:** The server must encode connection information into the ISN because it doesn't allocate memory for half-open connections.

The ISN (Initial Sequence Number) in SYNACK encodes:
- Client's IP and port
- Server's IP and port  
- Timestamp
- Cryptographic hash

When the client sends ACK(ISN+1), the server can:
1. Recompute what the ISN should be
2. Verify the ACK is legitimate
3. Reconstruct connection state without having stored it

This allows stateless handling of SYN until ACK arrives.

**b. Can attacker create half-open/fully-open connections by sending ACKs?**

**Half-open: NO**
- Server doesn't store half-open state when using SYN cookies
- No state to be "half-open"

**Fully-open: NO (not easily)**
- Attacker must send ACK with correct ISN+1
- Without seeing SYNACK, attacker doesn't know ISN
- ISN includes cryptographic hash, so can't be guessed
- Even if attacker guesses, server recomputes and verifies hash

However, if attacker can sniff SYNACK, they could complete handshake - but this requires being on-path, and attacker might as well do legitimate connection.

**Answer: No**, attacker cannot easily create connections without seeing SYNACK.

**c. If attacker collects many ISNs, can they force server to create many full connections?**

**Answer: NO**

Each ISN is tied to:
- Specific client IP/port
- Specific server IP/port  
- Timestamp (expires after ~30-60 seconds)
- Cryptographic hash using server secret

An ISN from connection (IP1, port1) → (ServerIP, port80) cannot be used for a different connection. The server will recompute the expected ISN for the incoming ACK's 5-tuple and reject if it doesn't match.

Additionally:
- ISNs have timestamps and expire
- Server secret changes periodically
- Old ISNs become invalid

**Conclusion:** Collecting ISNs doesn't help attacker create many connections. SYN cookies successfully defend against SYN flood attacks.

---

## P30. Buffer Size vs Throughput

**a. Why might increasing router buffer size decrease throughput?**

**Answer:** Larger buffers can decrease throughput due to:

**1. Bufferbloat:**
- Large buffers → long queuing delays
- TCP interprets delay as congestion signal (indirectly via timeouts)
- But with huge buffers, packets don't drop
- TCP may not reduce sending rate appropriately
- Multiple flows experience high RTT

**2. Timeout Issues:**
- Large buffers cause packets to sit in queue for long time
- If queuing delay > RTO (retransmission timeout)
- Sender times out and retransmits
- Original packet still in buffer!
- Both original and retransmitted packet delivered → waste bandwidth
- **Spurious retransmissions reduce throughput**

**3. Lock-out Effect:**
- With FIFO queues and large buffers
- Aggressive flows can monopolize buffer space
- Other flows get locked out
- Overall fairness and throughput suffer

**4. Memory Cost:**
- Very large buffers consume memory
- Fewer buffers available per port
- May reduce total router capacity

**Example:**
```
Small buffer (optimal):
RTT = 50ms, few retransmissions, throughput = 100 Mbps

Large buffer:
RTT = 500ms (due to queuing), many spurious retransmissions
Throughput = 70 Mbps (30% wasted on duplicates)
```

**b. If timeout adapts dynamically, would increasing buffer help?**

**Answer: Possibly, but with limitations**

**Adaptive timeout (like TCP's RTT estimation):**
- RTO adjusts to current RTT
- As queuing delay increases, RTO increases
- Fewer spurious retransmissions

**Benefits of larger buffer with adaptive timeout:**
- Absorbs traffic bursts without drops
- Smoother throughput during brief congestion
- Fewer packet losses → less congestion collapse

**Remaining problems:**
- **Still have bufferbloat:** High latency even without spurious retransmissions
- **Fairness issues:** Aggressive flows still monopolize buffers
- **ACK issues:** ACKs also delayed, slowing window growth
- **Slow convergence:** Takes longer to reach equilibrium

**Better approach:** 
- **Active Queue Management (AQM):** RED, CoDel
- **Optimal buffer sizing:** Rule of thumb = RTT × C / √N
  - C = link capacity
  - N = number of flows
- **Explicit Congestion Notification (ECN):** Signal congestion without dropping

**Conclusion:** Adaptive timeout helps, but doesn't solve fundamental bufferbloat problem. Moderate buffer sizes with AQM work better than very large buffers.